
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "tutorials/generated/1_linear_regression.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_tutorials_generated_1_linear_regression.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_tutorials_generated_1_linear_regression.py:


1 - Linear regression
=====================

.. GENERATED FROM PYTHON SOURCE LINES 9-39

|Open In Colab|

--------------

What we do in this notebook
---------------------------

Here we demonstrate use of CoFI on a simple **linear regression**
problem, where we fit a polynomial function to data, using three
different algorithms:

-  by solution of a linear system of equations,
-  by optimization of a data misfit function
-  by Bayesian sampling of a Likelihood multiplied by a prior.

--------------

Learning outcomes
-----------------

-  A demonstration of running CoFI for a class of parameter fitting
   problem. Example of a CoFI **template**.
-  A demonstration of how CoFI may be used to **experiment with
   different inference approaches** under a common interface.
-  A demonstration of CoFI’s **expandability** in that it may be used
   with pre-set, or user defined, misfits, likelihood or priors.

.. |Open In Colab| image:: https://img.shields.io/badge/open%20in-Colab-b5e2fa?logo=googlecolab&style=flat-square&color=ffd670
   :target: https://colab.research.google.com/github/inlab-geo/cofi-examples/blob/main/tutorials/1_linear_regression.ipynb


.. GENERATED FROM PYTHON SOURCE LINES 39-44

.. code-block:: default


    # Environment setup (uncomment code below)

    # !pip install -U cofi








.. GENERATED FROM PYTHON SOURCE LINES 49-54

Linear regression
-----------------

Lets start with some (x,y) data.


.. GENERATED FROM PYTHON SOURCE LINES 54-58

.. code-block:: default


    import numpy as np
    import matplotlib.pyplot as plt








.. GENERATED FROM PYTHON SOURCE LINES 60-79

.. code-block:: default


    # here is some (x,y) data
    data_x = np.array([1.1530612244897958, -0.07142857142857162, -1.7857142857142858, 
                    1.6428571428571423, -2.642857142857143, -1.0510204081632653, 
                    1.1530612244897958, -1.295918367346939, -0.806122448979592, 
                    -2.2755102040816326, -2.2755102040816326, -0.6836734693877551, 
                    0.7857142857142856, 1.2755102040816322, -0.6836734693877551, 
                    -3.2551020408163267, -0.9285714285714288, -3.377551020408163, 
                    -0.6836734693877551, 1.7653061224489797])

    data_y = np.array([-7.550931153863841, -6.060810406314714, 3.080063056254076, 
                    -4.499764131508964, 2.9462042659962333, -0.4645899453212615, 
                    -7.43068837808917, 1.6273774547833582, -0.05922697815443567, 
                    3.8462283231266903, 3.425851020301113, -0.05359797104829345, 
                    -10.235538857712598, -5.929113775071286, -1.1871766078924957, 
                    -4.124258811692425, 0.6969191559961637, -4.454022624935177, 
                    -2.352842192972056, -4.25145590011172])
    sigma = 1   # estimation on the data noise








.. GENERATED FROM PYTHON SOURCE LINES 84-86

And now lets plot the data.


.. GENERATED FROM PYTHON SOURCE LINES 86-94

.. code-block:: default


    def plot_data(sigma=None):
        if(sigma is None):
            plt.scatter(data_x, data_y, color="lightcoral", label="observed data")
        else:
            plt.errorbar(data_x, data_y, yerr=sigma, fmt='.',color="lightcoral",ecolor='lightgrey',ms=10)
    plot_data()




.. image-sg:: /tutorials/generated/images/sphx_glr_1_linear_regression_001.png
   :alt: 1 linear regression
   :srcset: /tutorials/generated/images/sphx_glr_1_linear_regression_001.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 99-136

Problem description
-------------------

To begin with, we will work with polynomial curves,

.. math:: y(x) = \sum_{j=0}^M m_j x^j\,.

Here, :math:`M` is the ‘order’ of the polynomial: if :math:`M=1` we have
a straight line with 2 parameters, if :math:`M=2` it will be a quadratic
with 3 parameters, and so on. The :math:`m_j, (j=0,\dots M)` are the
‘model coefficients’ that we seek to constrain from the data.

For this class of problem the forward operator takes the following form:

.. math::  \left(\begin{array}{c}y_0\\y_1\\\vdots\\y_N\end{array}\right) = \left(\begin{array}{ccc}1&x_0&x_0^2&x_0^3\\1&x_1&x_1^2&x_1^3\\\vdots&\vdots&\vdots\\1&x_N&x_N^2&x_N^3\end{array}\right)\left(\begin{array}{c}m_0\\m_1\\m_2\end{array}\right)

This clearly has the required general form,
:math:`\mathbf{d} =G{\mathbf m}`.

where:

-  :math:`\textbf{d}` is the vector of data values,
   (:math:`y_0,y_1,\dots,y_N`);
-  :math:`\textbf{m}` is the vector of model parameters,
   (:math:`m_0,m_1,m_2`);
-  :math:`G` is the basis matrix (or design matrix) of this linear
   regression problem (also called the **Jacobian** matrix for this
   linear problem).

We have a set of noisy data values, :math:`y_i (i=0,\dots,N)`, measured
at known locations, :math:`x_i (i=0,\dots,N)`, and wish to find the best
fit degree 3 polynomial.

The function that generated our data is : :math:`y=-6-5x+2x^2+x^3`, and
we have added Gaussian random noise, :math:`{\cal N}(0,\sigma^2)`, with
:math:`\sigma=1.0`.


.. GENERATED FROM PYTHON SOURCE LINES 139-142

We now build the Jacobian/G matrix for this problem and define a forward
function which simply multiplies :math:`\mathbf m` by :math:`G`.


.. GENERATED FROM PYTHON SOURCE LINES 142-154

.. code-block:: default


    nparams = 4 # Number of model parameters to be solved for

    def jacobian(x=data_x, n=nparams):
        return np.array([x**i for i in range(n)]).T

    def forward(model):
        return jacobian().dot(model)

    def Cd_inv(sigma=sigma, ndata=len(data_x)):
        return 1/sigma**2 * np.identity(ndata)








.. GENERATED FROM PYTHON SOURCE LINES 159-161

Define the true model for later.


.. GENERATED FROM PYTHON SOURCE LINES 161-168

.. code-block:: default


    # True model for plotting
    x = np.linspace(-3.5,2.5)              # x values to plot
    true_model = np.array([-6, -5, 2, 1])  # we know it for this case which will be useful later for comparison.

    true_y = jacobian(x,4).dot(true_model) # y values for true curve








.. GENERATED FROM PYTHON SOURCE LINES 173-176

Now lets plot the data with the curve from the true polynomial
coefficients.


.. GENERATED FROM PYTHON SOURCE LINES 176-196

.. code-block:: default


    # Some plotting utilities
    def plot_model(x,y, label, color=None):
        #x = np.linspace(-3.5,2.5)
        #y = jacobian(x).dot(model)
        plt.plot(x, y, color=color or "green", label=label)
        plt.xlabel("X")
        plt.ylabel("Y")
        plt.legend()

    def plot_models(models, label="Posterior samples", color="seagreen", alpha=0.1):
        x = np.linspace(-3.5,2.5)
        G = jacobian(x)
        plt.plot(x, G.dot(models[0]), color=color, label=label, alpha=alpha)
        for m in models:
            plt.plot(x, G.dot(m), color=color, alpha=alpha)
        plt.xlabel("X")
        plt.ylabel("Y")
        plt.legend()








.. GENERATED FROM PYTHON SOURCE LINES 198-202

.. code-block:: default


    plot_data(sigma=sigma)
    plot_model(x,true_y, "true model")




.. image-sg:: /tutorials/generated/images/sphx_glr_1_linear_regression_002.png
   :alt: 1 linear regression
   :srcset: /tutorials/generated/images/sphx_glr_1_linear_regression_002.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 207-210

Now we have the data and the forward model we can start to try and
estimate the coefficients of the polynomial from the data.


.. GENERATED FROM PYTHON SOURCE LINES 213-254

The structure of CoFI 
----------------------

In the workflow of ``cofi``, there are three main components:
``BaseProblem``, ``InversionOptions``, and ``Inversion``.

-  ``BaseProblem`` defines the inverse problem including any user
   supplied quantities such as data vector, number of model parameters
   and measure of fit between model predictions and data.
   ``python     inv_problem = BaseProblem()     inv_problem.set_objective(some_function_here)     inv_problem.set_jacobian(some_function_here)     inv_problem.set_initial_model(a_starting_point) # if needed, e.g. we are solving a nonlinear problem by optimization``

    

-  ``InversionOptions`` describes details about how one wants to run the
   inversion, including the backend tool and solver-specific parameters.
   It is based on the concept of a ``method`` and ``tool``.

   .. code:: python

      inv_options = InversionOptions()
      inv_options.suggest_solving_methods()
      inv_options.set_solving_method("matrix solvers")
      inv_options.suggest_tools()
      inv_options.set_tool("scipy.linalg.lstsq")
      inv_options.summary()

    

-  ``Inversion`` can be seen as an inversion engine that takes in the
   above two as information, and will produce an ``InversionResult``
   upon running.

   .. code:: python

      inv = Inversion(inv_problem, inv_options)
      result = inv.run()

Internally CoFI decides the nature of the problem from the quantities
set by the user and performs internal checks to ensure it has all that
it needs to solve a problem.


.. GENERATED FROM PYTHON SOURCE LINES 257-260

1. Linear system solver
-----------------------


.. GENERATED FROM PYTHON SOURCE LINES 260-263

.. code-block:: default


    from cofi import BaseProblem, InversionOptions, Inversion








.. GENERATED FROM PYTHON SOURCE LINES 268-271

Step 1. Define CoFI ``BaseProblem``
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


.. GENERATED FROM PYTHON SOURCE LINES 271-277

.. code-block:: default


    inv_problem = BaseProblem()
    inv_problem.set_data(data_y)
    inv_problem.set_jacobian(jacobian())
    inv_problem.set_data_covariance_inv(Cd_inv())








.. GENERATED FROM PYTHON SOURCE LINES 282-285

Step 2. Define CoFI ``InversionOptions``
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


.. GENERATED FROM PYTHON SOURCE LINES 285-288

.. code-block:: default


    inv_options = InversionOptions()








.. GENERATED FROM PYTHON SOURCE LINES 293-296

Using the information supplied, we can ask CoFI to suggest some solving
methods.


.. GENERATED FROM PYTHON SOURCE LINES 296-299

.. code-block:: default


    inv_options.suggest_solving_methods()





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    The following solving methods are supported:
    {'optimization', 'sampling', 'matrix solvers'}

    Use `suggest_tools()` to see a full list of backend tools for each method




.. GENERATED FROM PYTHON SOURCE LINES 304-306

We can ask CoFI to suggest some specific software tools as well.


.. GENERATED FROM PYTHON SOURCE LINES 306-309

.. code-block:: default


    inv_options.suggest_tools()





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Here's a complete list of inversion solvers supported by CoFI (grouped by methods):
    {
        "optimization": [
            "scipy.optimize.minimize",
            "scipy.optimize.least_squares",
            "torch.optim"
        ],
        "matrix solvers": [
            "scipy.linalg.lstsq",
            "cofi.simple_newton"
        ],
        "sampling": [
            "emcee"
        ]
    }




.. GENERATED FROM PYTHON SOURCE LINES 311-315

.. code-block:: default


    inv_options.set_solving_method("matrix solvers") # lets decide to use a matrix solver.
    inv_options.summary()





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    =============================
    Summary for inversion options
    =============================
    Solving method: matrix solvers
    Use `suggest_solving_methods()` to check available solving methods.
    -----------------------------
    Backend tool: `scipy.linalg.lstsq (by default)` - SciPy's wrapper function over LAPACK's linear least-squares solver, using 'gelsd', 'gelsy' (default), or 'gelss' as backend driver
    References: ['https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.lstsq.html', 'https://www.netlib.org/lapack/lug/node27.html']
    Use `suggest_tools()` to check available backend tools.
    -----------------------------
    Solver-specific parameters: None set
    Use `suggest_solver_params()` to check required/optional solver-specific parameters.




.. GENERATED FROM PYTHON SOURCE LINES 317-321

.. code-block:: default


    # below is optional, as this has already been the default tool under "linear least square"
    inv_options.set_tool("scipy.linalg.lstsq")








.. GENERATED FROM PYTHON SOURCE LINES 326-344

Step 3. Define CoFI ``Inversion`` and run
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Our choices so far have defined a linear parameter estimation problem
(without any regularization) to be solved within a least squares
framework. In this case the selection of a ``matrix solvers`` method
will mean we are calculating the standard least squares solution

.. math::


   m = (G^T C_d^{-1} G)^{-1} G^T C_d^{-1} d

and our choice of backend tool ``scipy.linalg.lstsq``, means that we
will employ scipy’s ``linalg`` package to perform the numerics.

Lets run CoFI.


.. GENERATED FROM PYTHON SOURCE LINES 344-348

.. code-block:: default


    inv = Inversion(inv_problem, inv_options)
    inv_result = inv.run()








.. GENERATED FROM PYTHON SOURCE LINES 350-354

.. code-block:: default


    print(f"The inversion result from `scipy.linalg.lstsq`: {inv_result.model}\n")
    inv_result.summary()





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    The inversion result from `scipy.linalg.lstsq`: [-5.71964359 -5.10903808  1.82553662  0.97472374]

    ============================
    Summary for inversion result
    ============================
    SUCCESS
    ----------------------------
    model: [-5.71964359 -5.10903808  1.82553662  0.97472374]
    sum_of_squared_residuals: []
    effective_rank: 4
    singular_values: [3765.51775745   69.19268194   16.27124488    3.85437889]
    model_covariance: [[ 0.19027447  0.05812534 -0.08168411 -0.02550866]
     [ 0.05812534  0.08673796 -0.03312809 -0.01812686]
     [-0.08168411 -0.03312809  0.05184851  0.01704165]
     [-0.02550866 -0.01812686  0.01704165  0.00676031]]




.. GENERATED FROM PYTHON SOURCE LINES 359-361

Lets plot the solution.


.. GENERATED FROM PYTHON SOURCE LINES 361-366

.. code-block:: default


    plot_data()
    plot_model(x,jacobian(x).dot(inv_result.model), "linear system solver", color="seagreen")
    plot_model(x,true_y, "true model", color="darkorange")




.. image-sg:: /tutorials/generated/images/sphx_glr_1_linear_regression_003.png
   :alt: 1 linear regression
   :srcset: /tutorials/generated/images/sphx_glr_1_linear_regression_003.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 371-396

2. Optimizer
------------

The same overdetermined linear problem,
:math:`\textbf{d} = G\textbf{m}`, with Gaussian data noise can also be
solved by minimising the squares of the residual of the linear
equations, e.g. :math:`\textbf{r}^T \textbf{C}_d^{-1}\textbf{r}` where
:math:`\textbf{r}=\textbf{d}-G\textbf{m}`. The above matrix solver
solution gives us the best data fitting model, but a direct optimisation
approach could also be used, say when the number of unknowns is large
and we do not wish, or are unable to provide the Jacobian function.

So we use a plain optimizer ``scipy.optimize.minimize`` to demonstrate
this ability.

.. raw:: html

   <!-- For this backend solver to run successfully, some additional information should be provided, otherwise
   you'll see an error to notify what additional information is required by the solver.

   There are several ways to provide the information needed to solve an inverse problem with 
   CoFI. In the example below we provide functions to calculate the data and the optional 
   regularisation. CoFI then generates the objective function for us based on the information 
   provided. The alternative to this would be to directly provide objective function to CoFI. -->


.. GENERATED FROM PYTHON SOURCE LINES 396-418

.. code-block:: default


    ######## CoFI BaseProblem - provide additional information
    inv_problem.set_initial_model(np.ones(nparams))
    inv_problem.set_forward(forward)
    inv_problem.set_data_misfit("squared error")

    # inv_problem.set_objective(your_own_misfit_function)    # (optionally) if you'd like to define your own misfit
    # inv_problem.set_gradient(your_own_gradient_of_misfit_function)    # (optionally) if you'd like to define your own misfit gradient

    ######## CoFI InversionOptions - set a different tool
    inv_options_2 = InversionOptions()
    inv_options_2.set_tool("scipy.optimize.minimize")
    inv_options_2.set_params(method="Nelder-Mead")

    ######## CoFI Inversion - run it
    inv_2 = Inversion(inv_problem, inv_options_2)
    inv_result_2 = inv_2.run()

    ######## CoFI InversionResult - check result
    print(f"The inversion result from `scipy.optimize.minimize`: {inv_result_2.model}\n")
    inv_result_2.summary()





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    The inversion result from `scipy.optimize.minimize`: [-5.71967431 -5.10913992  1.82556456  0.9747426 ]

    ============================
    Summary for inversion result
    ============================
    SUCCESS
    ----------------------------
    fun: 14.961508008942795
    nit: 193
    nfev: 330
    status: 0
    message: Optimization terminated successfully.
    final_simplex: (array([[-5.71967431, -5.10913992,  1.82556456,  0.9747426 ],
           [-5.71958302, -5.10907158,  1.8255083 ,  0.97472628],
           [-5.71969118, -5.10911404,  1.82556388,  0.97474474],
           [-5.7197282 , -5.10917942,  1.82554925,  0.97474097],
           [-5.71960767, -5.10913354,  1.82551338,  0.97473478]]), array([14.96150801, 14.96150804, 14.96150805, 14.9615082 , 14.96150821]))
    model: [-5.71967431 -5.10913992  1.82556456  0.9747426 ]




.. GENERATED FROM PYTHON SOURCE LINES 420-425

.. code-block:: default


    plot_data()
    plot_model(x,jacobian(x).dot(inv_result_2.model), "optimization solution", color="cornflowerblue")
    plot_model(x,true_y, "true model", color="darkorange")




.. image-sg:: /tutorials/generated/images/sphx_glr_1_linear_regression_004.png
   :alt: 1 linear regression
   :srcset: /tutorials/generated/images/sphx_glr_1_linear_regression_004.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 430-432

--------------


.. GENERATED FROM PYTHON SOURCE LINES 435-464

Challenge: Change the polynomial degree
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Try and replace the 3rd order polynomial with a 1st order polynomial
(i.e. :math:`M=1`) by adding the required commands below. What does the
plot looks like?

|Upload to Jamboard|

Start from code below:

::

   inv_problem = BaseProblem()
   inv_problem.set_data(data_y)
   inv_problem.set_jacobian(jacobian(n=<CHANGE ME>))
   inv_problem.set_data_covariance_inv(Cd_inv())
   inv_options.set_solving_method("matrix solvers") # lets decide to use a matrix solver.
   inv = Inversion(inv_problem, inv_options)
   inv_result = inv.run()

   print("Inferred curve with n = <CHANGE ME> ")
   plot_data()
   plot_model(x,jacobian(x,n=<CHANGE ME>).dot(inv_result.model), "optimization solution", color="cornflowerblue")
   plot_model(x,true_y, "true model", color="darkorange")

.. |Upload to Jamboard| image:: https://img.shields.io/badge/Click%20&%20upload%20your%20results%20to-Jamboard-lightgrey?logo=jamboard&style=for-the-badge&color=fcbf49&labelColor=edede9
   :target: https://jamboard.google.com/d/1Fu_vIhWIlDl-gs9gzSPBNXLjzj2CsS70fLMDN8-7Sew/edit?usp=sharing


.. GENERATED FROM PYTHON SOURCE LINES 464-469

.. code-block:: default


    # Copy the template above, Replace <CHANGE ME> with your answer










.. GENERATED FROM PYTHON SOURCE LINES 471-487

.. code-block:: default


    #@title Solution

    inv_problem = BaseProblem()
    inv_problem.set_data(data_y)
    inv_problem.set_jacobian(jacobian(n=2))
    inv_problem.set_data_covariance_inv(Cd_inv())
    inv_options.set_solving_method("matrix solvers") # lets decide to use a matrix solver.
    inv = Inversion(inv_problem, inv_options)
    inv_result = inv.run()

    print("Inferred curve with n = 2 ")
    plot_data()
    plot_model(x,jacobian(x,n=2).dot(inv_result.model), "optimization solution", color="cornflowerblue")
    plot_model(x,true_y, "true model", color="darkorange")




.. image-sg:: /tutorials/generated/images/sphx_glr_1_linear_regression_005.png
   :alt: 1 linear regression
   :srcset: /tutorials/generated/images/sphx_glr_1_linear_regression_005.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Inferred curve with n = 2 




.. GENERATED FROM PYTHON SOURCE LINES 492-494

--------------


.. GENERATED FROM PYTHON SOURCE LINES 497-500

3. Bayesian sampling
--------------------


.. GENERATED FROM PYTHON SOURCE LINES 503-526

Likelihood
~~~~~~~~~~

Since data errors follow a Gaussian in this example, we can define a
Likelihood function, :math:`p({\mathbf d}_{obs}| {\mathbf m})`.

.. math::


   p({\mathbf d}_{obs} | {\mathbf m}) \propto \exp \left\{- \frac{1}{2} ({\mathbf d}_{obs}-{\mathbf d}_{pred}({\mathbf m}))^T C_D^{-1} ({\mathbf d}_{obs}-{\mathbf d}_{pred}({\mathbf m})) \right\}

where :math:`{\mathbf d}_{obs}` represents the observed y values and
:math:`{\mathbf d}_{pred}({\mathbf m})` are those predicted by the
polynomial model :math:`({\mathbf m})`. The Likelihood is defined as the
probability of observing the data actually observed, given a model. In
practice we usually only need to evaluate the log of the Likelihood,
:math:`\log p({\mathbf d}_{obs} | {\mathbf m})`. To do so, we require
the inverse data covariance matrix describing the statistics of the
noise in the data, :math:`C_D^{-1}` . For this problem the data errors
are independent with identical standard deviation in noise for each
datum. Hence :math:`C_D^{-1} = \frac{1}{\sigma^2}I` where
:math:`\sigma=1`.


.. GENERATED FROM PYTHON SOURCE LINES 526-535

.. code-block:: default


    sigma = 1.0                                     # common noise standard deviation
    Cdinv = np.eye(len(data_y))/(sigma**2)      # inverse data covariance matrix

    def log_likelihood(model):
        y_synthetics = forward(model)
        residual = data_y - y_synthetics
        return -0.5 * residual @ (Cdinv @ residual).T








.. GENERATED FROM PYTHON SOURCE LINES 540-543

Note that the user could specify **any appropriate Likelihood function**
of their choosing here.


.. GENERATED FROM PYTHON SOURCE LINES 546-570

Prior
~~~~~

Bayesian sampling requires a prior probability density function. A
common problem with polynomial coefficients as model parameters is that
it is not at all obvious what a prior should be. Here we choose a
uniform prior with specified bounds

.. math::


   \begin{align}
   p({\mathbf m}) &= \frac{1}{V},\quad  l_i \le m_i \le u_i, \quad (i=1,\dots,M)\\
   \\
            &= 0, \quad {\rm otherwise},
   \end{align}

where :math:`l_i` and :math:`u_i` are lower and upper bounds on the
:math:`i`\ th model coefficient.

Here use the uniform distribution with
:math:`{\mathbf l}^T = (-10.,-10.,-10.,-10.)`, and
:math:`{\mathbf u}^T = (10.,10.,10.,10.)`.


.. GENERATED FROM PYTHON SOURCE LINES 570-579

.. code-block:: default


    m_lower_bound = np.ones(nparams) * (-10.)             # lower bound for uniform prior
    m_upper_bound = np.ones(nparams) * 10                 # upper bound for uniform prior

    def log_prior(model):    # uniform distribution
        for i in range(len(m_lower_bound)):
            if model[i] < m_lower_bound[i] or model[i] > m_upper_bound[i]: return -np.inf
        return 0.0 # model lies within bounds -> return log(1)








.. GENERATED FROM PYTHON SOURCE LINES 584-587

Note that the user could specify **any appropriate Prior PDF** of their
choosing here.


.. GENERATED FROM PYTHON SOURCE LINES 590-611

Bayesian sampling
~~~~~~~~~~~~~~~~~

In this aproach we sample a probability distribution rather than find a
single best fit solution. Bayes’ theorem tells us the the posterior
distribution is proportional to the Likelihood and the prior.

.. math:: p(\mathbf{m}|\mathbf{d}) = K p(\mathbf{d}|\mathbf{m})p(\mathbf{m})

where :math:`K` is some constant. Under the assumptions specified
:math:`p(\mathbf{m}|\mathbf{d})` gives a probability density of models
that are supported by the data. We seek to draw random samples from
:math:`p(\mathbf{m}|\mathbf{d})` over model space and then to make
inferences from the resulting ensemble of model parameters.

In this example we make use of *The Affine Invariant Markov chain Monte
Carlo (MCMC) Ensemble sampler* `Goodman and Weare
2010 <https://msp.org/camcos/2010/5-1/p04.xhtml>`__ to sample the
posterior distribution of the model. (See more details about
`emcee <https://emcee.readthedocs.io/en/stable/>`__).


.. GENERATED FROM PYTHON SOURCE LINES 614-621

Starting points for random walkers
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Now we define some hyperparameters (e.g. the number of walkers and
steps), and initialise the starting positions of walkers. We start all
walkers in a small ball about a chosen point :math:`(0, 0, 0, 0)`.


.. GENERATED FROM PYTHON SOURCE LINES 621-627

.. code-block:: default


    nwalkers = 32
    ndim = nparams
    nsteps = 10000
    walkers_start = np.zeros(nparams) + 1e-4 * np.random.randn(nwalkers, ndim)








.. GENERATED FROM PYTHON SOURCE LINES 632-635

Add the information and run with CoFI
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


.. GENERATED FROM PYTHON SOURCE LINES 635-654

.. code-block:: default


    ######## CoFI BaseProblem - provide additional information
    inv_problem.set_log_prior(log_prior)
    inv_problem.set_log_likelihood(log_likelihood)
    inv_problem.set_walkers_starting_pos(walkers_start)

    ######## CoFI InversionOptions - get a different tool
    inv_options_3 = InversionOptions()
    inv_options_3.set_tool("emcee")      # Here we use to Affine Invariant McMC sampler from Goodman and Weare (2010).
    inv_options_3.set_params(nwalkers=nwalkers, nsteps=nsteps, progress=True)

    ######## CoFI Inversion - run it
    inv_3 = Inversion(inv_problem, inv_options_3)
    inv_result_3 = inv_3.run()

    ######## CoFI InversionResult - check result
    print(f"The inversion result from `emcee`:")
    inv_result_3.summary()





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

      0%|          | 0/10000 [00:00<?, ?it/s]      1%|1         | 123/10000 [00:00<00:08, 1222.35it/s]      2%|2         | 246/10000 [00:00<00:08, 1179.03it/s]      4%|3         | 369/10000 [00:00<00:08, 1199.68it/s]      5%|4         | 491/10000 [00:00<00:07, 1203.87it/s]      6%|6         | 614/10000 [00:00<00:07, 1212.94it/s]      7%|7         | 736/10000 [00:00<00:07, 1214.76it/s]      9%|8         | 858/10000 [00:00<00:07, 1213.11it/s]     10%|9         | 982/10000 [00:00<00:07, 1220.54it/s]     11%|#1        | 1106/10000 [00:00<00:07, 1225.33it/s]     12%|#2        | 1230/10000 [00:01<00:07, 1228.21it/s]     14%|#3        | 1354/10000 [00:01<00:07, 1231.32it/s]     15%|#4        | 1478/10000 [00:01<00:06, 1224.91it/s]     16%|#6        | 1601/10000 [00:01<00:06, 1225.83it/s]     17%|#7        | 1725/10000 [00:01<00:06, 1227.72it/s]     18%|#8        | 1849/10000 [00:01<00:06, 1230.37it/s]     20%|#9        | 1973/10000 [00:01<00:06, 1232.50it/s]     21%|##        | 2097/10000 [00:01<00:06, 1233.79it/s]     22%|##2       | 2221/10000 [00:01<00:06, 1234.91it/s]     23%|##3       | 2345/10000 [00:01<00:06, 1228.66it/s]     25%|##4       | 2469/10000 [00:02<00:06, 1230.33it/s]     26%|##5       | 2593/10000 [00:02<00:06, 1232.08it/s]     27%|##7       | 2717/10000 [00:02<00:05, 1233.41it/s]     28%|##8       | 2841/10000 [00:02<00:05, 1233.86it/s]     30%|##9       | 2965/10000 [00:02<00:05, 1235.22it/s]     31%|###       | 3089/10000 [00:02<00:05, 1236.22it/s]     32%|###2      | 3213/10000 [00:02<00:05, 1222.97it/s]     33%|###3      | 3336/10000 [00:02<00:05, 1223.76it/s]     35%|###4      | 3460/10000 [00:02<00:05, 1227.70it/s]     36%|###5      | 3584/10000 [00:02<00:05, 1230.57it/s]     37%|###7      | 3708/10000 [00:03<00:05, 1231.56it/s]     38%|###8      | 3832/10000 [00:03<00:05, 1232.85it/s]     40%|###9      | 3956/10000 [00:03<00:04, 1232.85it/s]     41%|####      | 4080/10000 [00:03<00:04, 1234.13it/s]     42%|####2     | 4205/10000 [00:03<00:04, 1235.95it/s]     43%|####3     | 4329/10000 [00:03<00:04, 1236.78it/s]     45%|####4     | 4453/10000 [00:03<00:04, 1222.98it/s]     46%|####5     | 4576/10000 [00:03<00:04, 1223.13it/s]     47%|####6     | 4700/10000 [00:03<00:04, 1226.77it/s]     48%|####8     | 4824/10000 [00:03<00:04, 1229.31it/s]     49%|####9     | 4948/10000 [00:04<00:04, 1232.13it/s]     51%|#####     | 5072/10000 [00:04<00:03, 1233.39it/s]     52%|#####1    | 5196/10000 [00:04<00:03, 1234.39it/s]     53%|#####3    | 5320/10000 [00:04<00:03, 1212.37it/s]     54%|#####4    | 5444/10000 [00:04<00:03, 1217.77it/s]     56%|#####5    | 5568/10000 [00:04<00:03, 1223.04it/s]     57%|#####6    | 5692/10000 [00:04<00:03, 1227.61it/s]     58%|#####8    | 5816/10000 [00:04<00:03, 1230.15it/s]     59%|#####9    | 5940/10000 [00:04<00:03, 1230.89it/s]     61%|######    | 6064/10000 [00:04<00:03, 1232.74it/s]     62%|######1   | 6188/10000 [00:05<00:03, 1233.52it/s]     63%|######3   | 6312/10000 [00:05<00:02, 1234.87it/s]     64%|######4   | 6436/10000 [00:05<00:02, 1235.87it/s]     66%|######5   | 6560/10000 [00:05<00:02, 1216.83it/s]     67%|######6   | 6682/10000 [00:05<00:02, 1213.21it/s]     68%|######8   | 6806/10000 [00:05<00:02, 1219.17it/s]     69%|######9   | 6930/10000 [00:05<00:02, 1224.82it/s]     71%|#######   | 7054/10000 [00:05<00:02, 1228.71it/s]     72%|#######1  | 7177/10000 [00:05<00:02, 1226.71it/s]     73%|#######3  | 7301/10000 [00:05<00:02, 1228.11it/s]     74%|#######4  | 7425/10000 [00:06<00:02, 1230.86it/s]     75%|#######5  | 7549/10000 [00:06<00:01, 1233.15it/s]     77%|#######6  | 7673/10000 [00:06<00:01, 1234.68it/s]     78%|#######7  | 7797/10000 [00:06<00:01, 1235.20it/s]     79%|#######9  | 7921/10000 [00:06<00:01, 1235.05it/s]     80%|########  | 8045/10000 [00:06<00:01, 1235.81it/s]     82%|########1 | 8169/10000 [00:06<00:01, 1235.41it/s]     83%|########2 | 8293/10000 [00:06<00:01, 1236.22it/s]     84%|########4 | 8417/10000 [00:06<00:01, 1236.29it/s]     85%|########5 | 8541/10000 [00:06<00:01, 1236.33it/s]     87%|########6 | 8665/10000 [00:07<00:01, 1236.35it/s]     88%|########7 | 8789/10000 [00:07<00:00, 1236.94it/s]     89%|########9 | 8913/10000 [00:07<00:00, 1236.92it/s]     90%|######### | 9037/10000 [00:07<00:00, 1237.20it/s]     92%|#########1| 9161/10000 [00:07<00:00, 1236.94it/s]     93%|#########2| 9285/10000 [00:07<00:00, 1236.70it/s]     94%|#########4| 9409/10000 [00:07<00:00, 1235.63it/s]     95%|#########5| 9533/10000 [00:07<00:00, 1234.83it/s]     97%|#########6| 9657/10000 [00:07<00:00, 1233.66it/s]     98%|#########7| 9781/10000 [00:07<00:00, 1233.67it/s]     99%|#########9| 9905/10000 [00:08<00:00, 1233.69it/s]    100%|##########| 10000/10000 [00:08<00:00, 1229.05it/s]
    The inversion result from `emcee`:
    ============================
    Summary for inversion result
    ============================
    SUCCESS
    ----------------------------
    sampler: <emcee.ensemble.EnsembleSampler object>
    blob_names: ['log_likelihood', 'log_prior']




.. GENERATED FROM PYTHON SOURCE LINES 659-670

Post-sampling analysis
~~~~~~~~~~~~~~~~~~~~~~

By default the raw sampler resulting object is attached to ``cofi``\ ’s
inversion result.

Optionally, you can convert that into an ``arviz`` data structure to
have access to a range of analysis functions. (See more details in
`arviz
documentation <https://python.arviz.org/en/latest/index.html>`__).


.. GENERATED FROM PYTHON SOURCE LINES 670-679

.. code-block:: default


    import arviz as az

    labels = ["m0", "m1", "m2","m3"]

    sampler = inv_result_3.sampler
    az_idata = az.from_emcee(sampler, var_names=labels)
    # az_idata = inv_result_3.to_arviz()      # alternatively








.. GENERATED FROM PYTHON SOURCE LINES 681-684

.. code-block:: default


    az_idata.get("posterior")






.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <div><svg style="position: absolute; width: 0; height: 0; overflow: hidden">
    <defs>
    <symbol id="icon-database" viewBox="0 0 32 32">
    <path d="M16 0c-8.837 0-16 2.239-16 5v4c0 2.761 7.163 5 16 5s16-2.239 16-5v-4c0-2.761-7.163-5-16-5z"></path>
    <path d="M16 17c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z"></path>
    <path d="M16 26c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z"></path>
    </symbol>
    <symbol id="icon-file-text2" viewBox="0 0 32 32">
    <path d="M28.681 7.159c-0.694-0.947-1.662-2.053-2.724-3.116s-2.169-2.030-3.116-2.724c-1.612-1.182-2.393-1.319-2.841-1.319h-15.5c-1.378 0-2.5 1.121-2.5 2.5v27c0 1.378 1.122 2.5 2.5 2.5h23c1.378 0 2.5-1.122 2.5-2.5v-19.5c0-0.448-0.137-1.23-1.319-2.841zM24.543 5.457c0.959 0.959 1.712 1.825 2.268 2.543h-4.811v-4.811c0.718 0.556 1.584 1.309 2.543 2.268zM28 29.5c0 0.271-0.229 0.5-0.5 0.5h-23c-0.271 0-0.5-0.229-0.5-0.5v-27c0-0.271 0.229-0.5 0.5-0.5 0 0 15.499-0 15.5 0v7c0 0.552 0.448 1 1 1h7v19.5z"></path>
    <path d="M23 26h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z"></path>
    <path d="M23 22h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z"></path>
    <path d="M23 18h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z"></path>
    </symbol>
    </defs>
    </svg>
    <style>/* CSS stylesheet for displaying xarray objects in jupyterlab.
     *
     */

    :root {
      --xr-font-color0: var(--jp-content-font-color0, rgba(0, 0, 0, 1));
      --xr-font-color2: var(--jp-content-font-color2, rgba(0, 0, 0, 0.54));
      --xr-font-color3: var(--jp-content-font-color3, rgba(0, 0, 0, 0.38));
      --xr-border-color: var(--jp-border-color2, #e0e0e0);
      --xr-disabled-color: var(--jp-layout-color3, #bdbdbd);
      --xr-background-color: var(--jp-layout-color0, white);
      --xr-background-color-row-even: var(--jp-layout-color1, white);
      --xr-background-color-row-odd: var(--jp-layout-color2, #eeeeee);
    }

    html[theme=dark],
    body[data-theme=dark],
    body.vscode-dark {
      --xr-font-color0: rgba(255, 255, 255, 1);
      --xr-font-color2: rgba(255, 255, 255, 0.54);
      --xr-font-color3: rgba(255, 255, 255, 0.38);
      --xr-border-color: #1F1F1F;
      --xr-disabled-color: #515151;
      --xr-background-color: #111111;
      --xr-background-color-row-even: #111111;
      --xr-background-color-row-odd: #313131;
    }

    .xr-wrap {
      display: block !important;
      min-width: 300px;
      max-width: 700px;
    }

    .xr-text-repr-fallback {
      /* fallback to plain text repr when CSS is not injected (untrusted notebook) */
      display: none;
    }

    .xr-header {
      padding-top: 6px;
      padding-bottom: 6px;
      margin-bottom: 4px;
      border-bottom: solid 1px var(--xr-border-color);
    }

    .xr-header > div,
    .xr-header > ul {
      display: inline;
      margin-top: 0;
      margin-bottom: 0;
    }

    .xr-obj-type,
    .xr-array-name {
      margin-left: 2px;
      margin-right: 10px;
    }

    .xr-obj-type {
      color: var(--xr-font-color2);
    }

    .xr-sections {
      padding-left: 0 !important;
      display: grid;
      grid-template-columns: 150px auto auto 1fr 20px 20px;
    }

    .xr-section-item {
      display: contents;
    }

    .xr-section-item input {
      display: none;
    }

    .xr-section-item input + label {
      color: var(--xr-disabled-color);
    }

    .xr-section-item input:enabled + label {
      cursor: pointer;
      color: var(--xr-font-color2);
    }

    .xr-section-item input:enabled + label:hover {
      color: var(--xr-font-color0);
    }

    .xr-section-summary {
      grid-column: 1;
      color: var(--xr-font-color2);
      font-weight: 500;
    }

    .xr-section-summary > span {
      display: inline-block;
      padding-left: 0.5em;
    }

    .xr-section-summary-in:disabled + label {
      color: var(--xr-font-color2);
    }

    .xr-section-summary-in + label:before {
      display: inline-block;
      content: '►';
      font-size: 11px;
      width: 15px;
      text-align: center;
    }

    .xr-section-summary-in:disabled + label:before {
      color: var(--xr-disabled-color);
    }

    .xr-section-summary-in:checked + label:before {
      content: '▼';
    }

    .xr-section-summary-in:checked + label > span {
      display: none;
    }

    .xr-section-summary,
    .xr-section-inline-details {
      padding-top: 4px;
      padding-bottom: 4px;
    }

    .xr-section-inline-details {
      grid-column: 2 / -1;
    }

    .xr-section-details {
      display: none;
      grid-column: 1 / -1;
      margin-bottom: 5px;
    }

    .xr-section-summary-in:checked ~ .xr-section-details {
      display: contents;
    }

    .xr-array-wrap {
      grid-column: 1 / -1;
      display: grid;
      grid-template-columns: 20px auto;
    }

    .xr-array-wrap > label {
      grid-column: 1;
      vertical-align: top;
    }

    .xr-preview {
      color: var(--xr-font-color3);
    }

    .xr-array-preview,
    .xr-array-data {
      padding: 0 5px !important;
      grid-column: 2;
    }

    .xr-array-data,
    .xr-array-in:checked ~ .xr-array-preview {
      display: none;
    }

    .xr-array-in:checked ~ .xr-array-data,
    .xr-array-preview {
      display: inline-block;
    }

    .xr-dim-list {
      display: inline-block !important;
      list-style: none;
      padding: 0 !important;
      margin: 0;
    }

    .xr-dim-list li {
      display: inline-block;
      padding: 0;
      margin: 0;
    }

    .xr-dim-list:before {
      content: '(';
    }

    .xr-dim-list:after {
      content: ')';
    }

    .xr-dim-list li:not(:last-child):after {
      content: ',';
      padding-right: 5px;
    }

    .xr-has-index {
      font-weight: bold;
    }

    .xr-var-list,
    .xr-var-item {
      display: contents;
    }

    .xr-var-item > div,
    .xr-var-item label,
    .xr-var-item > .xr-var-name span {
      background-color: var(--xr-background-color-row-even);
      margin-bottom: 0;
    }

    .xr-var-item > .xr-var-name:hover span {
      padding-right: 5px;
    }

    .xr-var-list > li:nth-child(odd) > div,
    .xr-var-list > li:nth-child(odd) > label,
    .xr-var-list > li:nth-child(odd) > .xr-var-name span {
      background-color: var(--xr-background-color-row-odd);
    }

    .xr-var-name {
      grid-column: 1;
    }

    .xr-var-dims {
      grid-column: 2;
    }

    .xr-var-dtype {
      grid-column: 3;
      text-align: right;
      color: var(--xr-font-color2);
    }

    .xr-var-preview {
      grid-column: 4;
    }

    .xr-var-name,
    .xr-var-dims,
    .xr-var-dtype,
    .xr-preview,
    .xr-attrs dt {
      white-space: nowrap;
      overflow: hidden;
      text-overflow: ellipsis;
      padding-right: 10px;
    }

    .xr-var-name:hover,
    .xr-var-dims:hover,
    .xr-var-dtype:hover,
    .xr-attrs dt:hover {
      overflow: visible;
      width: auto;
      z-index: 1;
    }

    .xr-var-attrs,
    .xr-var-data {
      display: none;
      background-color: var(--xr-background-color) !important;
      padding-bottom: 5px !important;
    }

    .xr-var-attrs-in:checked ~ .xr-var-attrs,
    .xr-var-data-in:checked ~ .xr-var-data {
      display: block;
    }

    .xr-var-data > table {
      float: right;
    }

    .xr-var-name span,
    .xr-var-data,
    .xr-attrs {
      padding-left: 25px !important;
    }

    .xr-attrs,
    .xr-var-attrs,
    .xr-var-data {
      grid-column: 1 / -1;
    }

    dl.xr-attrs {
      padding: 0;
      margin: 0;
      display: grid;
      grid-template-columns: 125px auto;
    }

    .xr-attrs dt,
    .xr-attrs dd {
      padding: 0;
      margin: 0;
      float: left;
      padding-right: 10px;
      width: auto;
    }

    .xr-attrs dt {
      font-weight: normal;
      grid-column: 1;
    }

    .xr-attrs dt:hover span {
      display: inline-block;
      background: var(--xr-background-color);
      padding-right: 10px;
    }

    .xr-attrs dd {
      grid-column: 2;
      white-space: pre-wrap;
      word-break: break-all;
    }

    .xr-icon-database,
    .xr-icon-file-text2 {
      display: inline-block;
      vertical-align: middle;
      width: 1em;
      height: 1.5em !important;
      stroke-width: 0;
      stroke: currentColor;
      fill: currentColor;
    }
    </style><pre class='xr-text-repr-fallback'>&lt;xarray.Dataset&gt;
    Dimensions:  (chain: 32, draw: 10000)
    Coordinates:
      * chain    (chain) int64 0 1 2 3 4 5 6 7 8 9 ... 22 23 24 25 26 27 28 29 30 31
      * draw     (draw) int64 0 1 2 3 4 5 6 7 ... 9993 9994 9995 9996 9997 9998 9999
    Data variables:
        m0       (chain, draw) float64 -3.795e-05 -2.444e-05 ... -5.835 -5.939
        m1       (chain, draw) float64 -7.513e-05 -8.467e-05 ... -4.35 -4.621
        m2       (chain, draw) float64 -4.962e-05 -8.385e-05 ... 2.049 1.982
        m3       (chain, draw) float64 -0.0001157 -0.000168 ... 0.9438 0.9537
    Attributes:
        created_at:                 2022-11-24T03:05:04.156151
        arviz_version:              0.12.1
        inference_library:          emcee
        inference_library_version:  3.1.2</pre><div class='xr-wrap' style='display:none'><div class='xr-header'><div class='xr-obj-type'>xarray.Dataset</div></div><ul class='xr-sections'><li class='xr-section-item'><input id='section-b1f29ee3-6d8b-49f4-87a8-0222e1325183' class='xr-section-summary-in' type='checkbox' disabled ><label for='section-b1f29ee3-6d8b-49f4-87a8-0222e1325183' class='xr-section-summary'  title='Expand/collapse section'>Dimensions:</label><div class='xr-section-inline-details'><ul class='xr-dim-list'><li><span class='xr-has-index'>chain</span>: 32</li><li><span class='xr-has-index'>draw</span>: 10000</li></ul></div><div class='xr-section-details'></div></li><li class='xr-section-item'><input id='section-d83d3980-a52e-4661-b7e8-56396d866e93' class='xr-section-summary-in' type='checkbox'  checked><label for='section-d83d3980-a52e-4661-b7e8-56396d866e93' class='xr-section-summary' >Coordinates: <span>(2)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>chain</span></div><div class='xr-var-dims'>(chain)</div><div class='xr-var-dtype'>int64</div><div class='xr-var-preview xr-preview'>0 1 2 3 4 5 6 ... 26 27 28 29 30 31</div><input id='attrs-bf4a5596-05a4-4304-b33b-f244a1ac8729' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-bf4a5596-05a4-4304-b33b-f244a1ac8729' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-e74c1217-e047-459f-bafe-f017953b1388' class='xr-var-data-in' type='checkbox'><label for='data-e74c1217-e047-459f-bafe-f017953b1388' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
           18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31])</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>draw</span></div><div class='xr-var-dims'>(draw)</div><div class='xr-var-dtype'>int64</div><div class='xr-var-preview xr-preview'>0 1 2 3 4 ... 9996 9997 9998 9999</div><input id='attrs-67d53c62-51da-454d-a573-8d3b8ddf2ebf' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-67d53c62-51da-454d-a573-8d3b8ddf2ebf' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-61b9ac3e-a37a-42fd-901b-cb17069be769' class='xr-var-data-in' type='checkbox'><label for='data-61b9ac3e-a37a-42fd-901b-cb17069be769' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array([   0,    1,    2, ..., 9997, 9998, 9999])</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-211365c5-09d0-4af2-97dc-434b78a4bccb' class='xr-section-summary-in' type='checkbox'  checked><label for='section-211365c5-09d0-4af2-97dc-434b78a4bccb' class='xr-section-summary' >Data variables: <span>(4)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-var-name'><span>m0</span></div><div class='xr-var-dims'>(chain, draw)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>-3.795e-05 -2.444e-05 ... -5.939</div><input id='attrs-a5870c9e-0665-4b8f-ac32-af609c46969b' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-a5870c9e-0665-4b8f-ac32-af609c46969b' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-d30b4bf7-d328-450e-80e6-5c1b6694d1d6' class='xr-var-data-in' type='checkbox'><label for='data-d30b4bf7-d328-450e-80e6-5c1b6694d1d6' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array([[-3.79457962e-05, -2.44440896e-05, -2.44440896e-05, ...,
            -5.64133063e+00, -5.64133063e+00, -5.50629509e+00],
           [-8.54932197e-05, -8.54932197e-05, -9.65959405e-05, ...,
            -5.95695928e+00, -5.95695928e+00, -5.95695928e+00],
           [-7.93503234e-05, -7.93503234e-05, -8.40807663e-05, ...,
            -5.67163490e+00, -5.59575029e+00, -5.43788094e+00],
           ...,
           [-4.57373460e-05, -1.62870890e-05, -1.62870890e-05, ...,
            -5.44860623e+00, -5.37327435e+00, -5.35135485e+00],
           [-7.74393717e-05, -9.89153583e-05, -9.89153583e-05, ...,
            -6.29398097e+00, -6.27666248e+00, -6.38141769e+00],
           [-7.50488665e-05, -1.04955794e-04, -1.04955794e-04, ...,
            -5.86734915e+00, -5.83471219e+00, -5.93873313e+00]])</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>m1</span></div><div class='xr-var-dims'>(chain, draw)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>-7.513e-05 -8.467e-05 ... -4.621</div><input id='attrs-612a49da-b6ac-48cf-8f29-08585b12d69f' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-612a49da-b6ac-48cf-8f29-08585b12d69f' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-46792d15-b9d3-4ce6-bff9-e5233d506b5f' class='xr-var-data-in' type='checkbox'><label for='data-46792d15-b9d3-4ce6-bff9-e5233d506b5f' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array([[-7.51297375e-05, -8.46706488e-05, -8.46706488e-05, ...,
            -5.19440097e+00, -5.19440097e+00, -5.21985752e+00],
           [-4.15305960e-05, -4.15305960e-05,  6.69705485e-05, ...,
            -5.09103914e+00, -5.09103914e+00, -5.09103914e+00],
           [ 4.62174821e-05,  4.62174821e-05,  4.68969231e-05, ...,
            -4.76813476e+00, -4.74013228e+00, -4.54265695e+00],
           ...,
           [ 1.20180201e-04,  1.84983181e-04,  1.84983181e-04, ...,
            -4.95328305e+00, -5.01582016e+00, -5.03976850e+00],
           [-5.78400837e-04, -8.52071764e-04, -8.52071764e-04, ...,
            -5.49148819e+00, -5.41731738e+00, -5.54668508e+00],
           [-1.88576663e-05, -2.90000308e-05, -2.90000308e-05, ...,
            -4.47241476e+00, -4.34981914e+00, -4.62061048e+00]])</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>m2</span></div><div class='xr-var-dims'>(chain, draw)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>-4.962e-05 -8.385e-05 ... 1.982</div><input id='attrs-8674fdb6-e05b-4bcd-9834-f52852777046' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-8674fdb6-e05b-4bcd-9834-f52852777046' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-23e74982-b165-4b4f-ab1d-460cbbe4154e' class='xr-var-data-in' type='checkbox'><label for='data-23e74982-b165-4b4f-ab1d-460cbbe4154e' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array([[-4.96194317e-05, -8.38484763e-05, -8.38484763e-05, ...,
             1.87732390e+00,  1.87732390e+00,  1.73779647e+00],
           [ 7.09210988e-05,  7.09210988e-05,  2.04297484e-04, ...,
             1.95918530e+00,  1.95918530e+00,  1.95918530e+00],
           [ 6.73284373e-05,  6.73284373e-05,  6.88410835e-05, ...,
             1.85531376e+00,  1.84720193e+00,  1.83750795e+00],
           ...,
           [-4.38323939e-05, -1.41226772e-04, -1.41226772e-04, ...,
             1.51583442e+00,  1.40116928e+00,  1.58992614e+00],
           [-1.35089804e-04, -1.81567252e-04, -1.81567252e-04, ...,
             1.99153235e+00,  1.97646167e+00,  2.15836213e+00],
           [ 4.90159035e-05,  2.32415642e-04,  2.32415642e-04, ...,
             2.00268021e+00,  2.04900116e+00,  1.98172224e+00]])</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>m3</span></div><div class='xr-var-dims'>(chain, draw)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>-0.0001157 -0.000168 ... 0.9537</div><input id='attrs-ef190a6a-444f-4b12-a237-9a05d288ceed' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-ef190a6a-444f-4b12-a237-9a05d288ceed' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-27550ff8-9df5-4fb1-9a7c-417188d825a2' class='xr-var-data-in' type='checkbox'><label for='data-27550ff8-9df5-4fb1-9a7c-417188d825a2' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array([[-1.15717416e-04, -1.68024772e-04, -1.68024772e-04, ...,
             9.64716886e-01,  9.64716886e-01,  9.39322123e-01],
           [ 6.84874428e-05,  6.84874428e-05,  7.76938061e-05, ...,
             1.01194902e+00,  1.01194902e+00,  1.01194902e+00],
           [ 7.68845130e-05,  7.68845130e-05,  8.10885699e-05, ...,
             9.50894347e-01,  9.44731834e-01,  9.25735155e-01],
           ...,
           [ 4.56169551e-05,  1.82216548e-05,  1.82216548e-05, ...,
             8.57711497e-01,  8.26237343e-01,  9.08267380e-01],
           [-2.33341743e-04, -2.97304005e-04, -2.97304005e-04, ...,
             1.07464848e+00,  1.05841132e+00,  1.10321197e+00],
           [-1.16962863e-05, -4.18375546e-06, -4.18375546e-06, ...,
             9.44735626e-01,  9.43799728e-01,  9.53744393e-01]])</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-80c6898b-8edb-40a8-9755-87497fd9a36f' class='xr-section-summary-in' type='checkbox'  checked><label for='section-80c6898b-8edb-40a8-9755-87497fd9a36f' class='xr-section-summary' >Attributes: <span>(4)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><dl class='xr-attrs'><dt><span>created_at :</span></dt><dd>2022-11-24T03:05:04.156151</dd><dt><span>arviz_version :</span></dt><dd>0.12.1</dd><dt><span>inference_library :</span></dt><dd>emcee</dd><dt><span>inference_library_version :</span></dt><dd>3.1.2</dd></dl></div></li></ul></div></div>
    </div>
    <br />
    <br />

.. GENERATED FROM PYTHON SOURCE LINES 686-700

.. code-block:: default


    # a standard `trace` plot
    axes = az.plot_trace(az_idata, backend_kwargs={"constrained_layout":True}); 

    # add legends
    for i, axes_pair in enumerate(axes):
        ax1 = axes_pair[0]
        ax2 = axes_pair[1]
        ax1.axvline(true_model[i], linestyle='dotted', color='red')
        ax1.set_xlabel("parameter value")
        ax1.set_ylabel("density value")
        ax2.set_xlabel("number of iterations")
        ax2.set_ylabel("parameter value")




.. image-sg:: /tutorials/generated/images/sphx_glr_1_linear_regression_006.png
   :alt: m0, m0, m1, m1, m2, m2, m3, m3
   :srcset: /tutorials/generated/images/sphx_glr_1_linear_regression_006.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 702-706

.. code-block:: default


    tau = sampler.get_autocorr_time()
    print(f"autocorrelation time: {tau}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    autocorrelation time: [84.40934454 63.60822448 67.53668676 66.398074  ]




.. GENERATED FROM PYTHON SOURCE LINES 708-733

.. code-block:: default


    # a Corner plot

    fig, axes = plt.subplots(nparams, nparams, figsize=(12,8))

    if(False): # if we are plotting the model ensemble use this
        az.plot_pair(
            az_idata.sel(draw=slice(300,None)), 
            marginals=True, 
            reference_values=dict(zip([f"m{i}" for i in range(4)], true_model.tolist())),
            ax=axes,
        );
    else: # if we wish to plot a kernel density plot then use this option
        az.plot_pair(
            az_idata.sel(draw=slice(300,None)), 
            marginals=True, 
            reference_values=dict(zip([f"m{i}" for i in range(4)], true_model.tolist())),
            kind="kde",
            kde_kwargs={
                "hdi_probs": [0.3, 0.6, 0.9],  # Plot 30%, 60% and 90% HDI contours
                "contourf_kwargs": {"cmap": "Blues"},
            },
            ax=axes,
        );




.. image-sg:: /tutorials/generated/images/sphx_glr_1_linear_regression_007.png
   :alt: 1 linear regression
   :srcset: /tutorials/generated/images/sphx_glr_1_linear_regression_007.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 738-741

Now we plot the predicted curves for the posterior ensemble of
solutions.


.. GENERATED FROM PYTHON SOURCE LINES 741-749

.. code-block:: default


    flat_samples = sampler.get_chain(discard=300, thin=30, flat=True)
    inds = np.random.randint(len(flat_samples), size=100) # get a random selection from posterior ensemble

    plot_data()
    plot_models(flat_samples[inds])
    plot_model(x,true_y, "True model", color="darkorange")




.. image-sg:: /tutorials/generated/images/sphx_glr_1_linear_regression_008.png
   :alt: 1 linear regression
   :srcset: /tutorials/generated/images/sphx_glr_1_linear_regression_008.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 754-757

Expected values, credible intervals and model covariance matrix from the ensemble
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^


.. GENERATED FROM PYTHON SOURCE LINES 757-763

.. code-block:: default


    print("\n Expected value and 95% credible intervals ")
    for i in range(ndim):
        mcmc = np.percentile(flat_samples[:, i], [5, 50, 95])
        print(" {} {:7.3f} [{:7.3f}, {:7.3f}]".format(labels[i],mcmc[1],mcmc[0],mcmc[2]))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


     Expected value and 95% credible intervals 
     m0  -5.722 [ -6.433,  -5.004]
     m1  -5.109 [ -5.593,  -4.616]
     m2   1.826 [  1.456,   2.196]
     m3   0.974 [  0.838,   1.108]




.. GENERATED FROM PYTHON SOURCE LINES 765-773

.. code-block:: default


    CMpost = np.cov(flat_samples.T)
    CM_std= np.std(flat_samples,axis=0)
    print('Posterior model covariance matrix\n',CMpost)
    print('\n Posterior estimate of model standard deviations in each parameter')
    for i in range(ndim):
        print("    {} {:7.4f}".format(labels[i],CM_std[i]))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Posterior model covariance matrix
     [[ 0.18853668  0.05980443 -0.07956224 -0.02514067]
     [ 0.05980443  0.08789096 -0.03334047 -0.01827957]
     [-0.07956224 -0.03334047  0.05041612  0.01672363]
     [-0.02514067 -0.01827957  0.01672363  0.00670353]]

     Posterior estimate of model standard deviations in each parameter
        m0  0.4342
        m1  0.2964
        m2  0.2245
        m3  0.0819




.. GENERATED FROM PYTHON SOURCE LINES 778-780

--------------


.. GENERATED FROM PYTHON SOURCE LINES 783-786

Challenge: Change the prior model bounds
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


.. GENERATED FROM PYTHON SOURCE LINES 789-839

Replace the previous prior bounds to new values

The original uniform bounds had

:math:`{\mathbf l}^T = (-10.,-10.,-10.,-10.)`, and
:math:`{\mathbf u}^T = (10.,10.,10.,10.)`.

Lets replace with

:math:`{\mathbf l}^T = (-1.,-10.,-10.,-10.)`, and
:math:`{\mathbf u}^T = (2.,10.,10.,10.)`.

We have only changed the bounds of the first parameter. However since
the true value of constant term was 6, these bounds are now inconsistent
with the true model.

What does this do to the posterior distribution?

|Upload to Jamboard|

Start from the code template below:

::

   m_lower_bound = <CHANGE ME>             # lower bound for uniform prior
   m_upper_bound = <CHANGE ME>             # upper bound for uniform prior

   def log_prior(model):    # uniform distribution
       for i in range(len(m_lower_bound)):
           if model[i] < m_lower_bound[i] or model[i] > m_upper_bound[i]: return -np.inf
       return 0.0 # model lies within bounds -> return log(1)

   ######## CoFI BaseProblem - update information
   inv_problem.set_log_prior(log_prior)

   ######## CoFI Inversion - run it
   inv_4 = Inversion(inv_problem, inv_options_3)
   inv_result_4 = inv_4.run()

   flat_samples = inv_result_4.sampler.get_chain(discard=300, thin=30, flat=True)
   inds = np.random.randint(len(flat_samples), size=100) # get a random selection from posterior ensemble

   print("Resulting samples with prior model lower bounds of <CHANGE ME>, upper bounds of <CHANGE ME>")
   plot_data()
   plot_models(flat_samples[inds])
   plot_model(x, true_y, "True model", color="darkorange")

.. |Upload to Jamboard| image:: https://img.shields.io/badge/Click%20&%20upload%20your%20results%20to-Jamboard-lightgrey?logo=jamboard&style=for-the-badge&color=fcbf49&labelColor=edede9
   :target: https://jamboard.google.com/d/1h_O8PNuHzpyH2zQUraqiMT4SQR0TMhUmiZzFn_HMZl4/edit?usp=sharing


.. GENERATED FROM PYTHON SOURCE LINES 839-844

.. code-block:: default


    # Copy the template above, Replace <CHANGE ME> with your answer










.. GENERATED FROM PYTHON SOURCE LINES 846-872

.. code-block:: default


    #@title Solution

    m_lower_bound = np.array([-1,-10,-10,-10])             # lower bound for uniform prior
    m_upper_bound = np.array([2,10,10,10])                 # upper bound for uniform prior

    def log_prior(model):    # uniform distribution
        for i in range(len(m_lower_bound)):
            if model[i] < m_lower_bound[i] or model[i] > m_upper_bound[i]: return -np.inf
        return 0.0 # model lies within bounds -> return log(1)

    ######## CoFI BaseProblem - update information
    inv_problem.set_log_prior(log_prior)

    ######## CoFI Inversion - run it
    inv_4 = Inversion(inv_problem, inv_options_3)
    inv_result_4 = inv_4.run()

    flat_samples = inv_result_4.sampler.get_chain(discard=300, thin=30, flat=True)
    inds = np.random.randint(len(flat_samples), size=100) # get a random selection from posterior ensemble

    print("Resulting samples with prior model lower bounds of [-1,-10,-10,-10], upper bounds of [2,10,10,10]")
    plot_data()
    plot_models(flat_samples[inds])
    plot_model(x, true_y, "True model", color="darkorange")




.. image-sg:: /tutorials/generated/images/sphx_glr_1_linear_regression_009.png
   :alt: 1 linear regression
   :srcset: /tutorials/generated/images/sphx_glr_1_linear_regression_009.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

      0%|          | 0/10000 [00:00<?, ?it/s]      1%|1         | 118/10000 [00:00<00:08, 1172.47it/s]      2%|2         | 239/10000 [00:00<00:08, 1192.39it/s]      4%|3         | 361/10000 [00:00<00:08, 1203.74it/s]      5%|4         | 482/10000 [00:00<00:07, 1189.79it/s]      6%|6         | 605/10000 [00:00<00:07, 1200.80it/s]      7%|7         | 726/10000 [00:00<00:07, 1203.63it/s]      8%|8         | 849/10000 [00:00<00:07, 1209.44it/s]     10%|9         | 970/10000 [00:00<00:07, 1179.51it/s]     11%|#         | 1091/10000 [00:00<00:07, 1188.46it/s]     12%|#2        | 1210/10000 [00:01<00:07, 1183.56it/s]     13%|#3        | 1329/10000 [00:01<00:07, 1168.22it/s]     14%|#4        | 1448/10000 [00:01<00:07, 1174.11it/s]     16%|#5        | 1566/10000 [00:01<00:07, 1175.67it/s]     17%|#6        | 1684/10000 [00:01<00:07, 1161.64it/s]     18%|#8        | 1804/10000 [00:01<00:06, 1170.88it/s]     19%|#9        | 1922/10000 [00:01<00:06, 1173.16it/s]     20%|##        | 2044/10000 [00:01<00:06, 1186.25it/s]     22%|##1       | 2163/10000 [00:01<00:06, 1171.25it/s]     23%|##2       | 2284/10000 [00:01<00:06, 1181.65it/s]     24%|##4       | 2405/10000 [00:02<00:06, 1188.34it/s]     25%|##5       | 2524/10000 [00:02<00:06, 1179.11it/s]     26%|##6       | 2646/10000 [00:02<00:06, 1189.29it/s]     28%|##7       | 2769/10000 [00:02<00:06, 1198.71it/s]     29%|##8       | 2892/10000 [00:02<00:05, 1205.58it/s]     30%|###       | 3013/10000 [00:02<00:05, 1193.50it/s]     31%|###1      | 3136/10000 [00:02<00:05, 1202.02it/s]     33%|###2      | 3258/10000 [00:02<00:05, 1206.76it/s]     34%|###3      | 3380/10000 [00:02<00:05, 1210.32it/s]     35%|###5      | 3503/10000 [00:02<00:05, 1213.60it/s]     36%|###6      | 3626/10000 [00:03<00:05, 1216.00it/s]     37%|###7      | 3749/10000 [00:03<00:05, 1218.15it/s]     39%|###8      | 3872/10000 [00:03<00:05, 1218.92it/s]     40%|###9      | 3994/10000 [00:03<00:04, 1218.63it/s]     41%|####1     | 4117/10000 [00:03<00:04, 1219.65it/s]     42%|####2     | 4239/10000 [00:03<00:04, 1219.50it/s]     44%|####3     | 4362/10000 [00:03<00:04, 1220.22it/s]     45%|####4     | 4485/10000 [00:03<00:04, 1220.19it/s]     46%|####6     | 4608/10000 [00:03<00:04, 1218.01it/s]     47%|####7     | 4730/10000 [00:03<00:04, 1214.91it/s]     49%|####8     | 4853/10000 [00:04<00:04, 1216.71it/s]     50%|####9     | 4976/10000 [00:04<00:04, 1218.42it/s]     51%|#####     | 5099/10000 [00:04<00:04, 1220.15it/s]     52%|#####2    | 5222/10000 [00:04<00:03, 1220.93it/s]     53%|#####3    | 5345/10000 [00:04<00:03, 1221.24it/s]     55%|#####4    | 5468/10000 [00:04<00:03, 1221.14it/s]     56%|#####5    | 5591/10000 [00:04<00:03, 1221.71it/s]     57%|#####7    | 5714/10000 [00:04<00:03, 1222.02it/s]     58%|#####8    | 5837/10000 [00:04<00:03, 1222.07it/s]     60%|#####9    | 5960/10000 [00:04<00:03, 1221.93it/s]     61%|######    | 6083/10000 [00:05<00:03, 1221.70it/s]     62%|######2   | 6206/10000 [00:05<00:03, 1222.07it/s]     63%|######3   | 6329/10000 [00:05<00:03, 1222.45it/s]     65%|######4   | 6452/10000 [00:05<00:02, 1222.22it/s]     66%|######5   | 6575/10000 [00:05<00:02, 1222.55it/s]     67%|######6   | 6698/10000 [00:05<00:02, 1222.38it/s]     68%|######8   | 6821/10000 [00:05<00:02, 1221.25it/s]     69%|######9   | 6944/10000 [00:05<00:02, 1221.20it/s]     71%|#######   | 7067/10000 [00:05<00:02, 1220.46it/s]     72%|#######1  | 7190/10000 [00:05<00:02, 1221.10it/s]     73%|#######3  | 7313/10000 [00:06<00:02, 1221.61it/s]     74%|#######4  | 7436/10000 [00:06<00:02, 1221.75it/s]     76%|#######5  | 7559/10000 [00:06<00:02, 1215.73it/s]     77%|#######6  | 7681/10000 [00:06<00:01, 1215.09it/s]     78%|#######8  | 7803/10000 [00:06<00:01, 1215.96it/s]     79%|#######9  | 7926/10000 [00:06<00:01, 1218.20it/s]     80%|########  | 8049/10000 [00:06<00:01, 1219.67it/s]     82%|########1 | 8172/10000 [00:06<00:01, 1220.60it/s]     83%|########2 | 8295/10000 [00:06<00:01, 1221.60it/s]     84%|########4 | 8418/10000 [00:06<00:01, 1221.88it/s]     85%|########5 | 8541/10000 [00:07<00:01, 1220.26it/s]     87%|########6 | 8664/10000 [00:07<00:01, 1220.35it/s]     88%|########7 | 8787/10000 [00:07<00:00, 1220.70it/s]     89%|########9 | 8910/10000 [00:07<00:00, 1221.40it/s]     90%|######### | 9033/10000 [00:07<00:00, 1220.94it/s]     92%|#########1| 9156/10000 [00:07<00:00, 1222.50it/s]     93%|#########2| 9279/10000 [00:07<00:00, 1222.90it/s]     94%|#########4| 9402/10000 [00:07<00:00, 1223.02it/s]     95%|#########5| 9525/10000 [00:07<00:00, 1220.43it/s]     96%|#########6| 9648/10000 [00:07<00:00, 1219.86it/s]     98%|#########7| 9770/10000 [00:08<00:00, 1216.67it/s]     99%|#########8| 9892/10000 [00:08<00:00, 1217.25it/s]    100%|##########| 10000/10000 [00:08<00:00, 1209.64it/s]
    Resulting samples with prior model lower bounds of [-1,-10,-10,-10], upper bounds of [2,10,10,10]




.. GENERATED FROM PYTHON SOURCE LINES 877-879

Why do you think the posterior distribution looks like this?


.. GENERATED FROM PYTHON SOURCE LINES 882-884

--------------


.. GENERATED FROM PYTHON SOURCE LINES 887-897

Challenge: Change the data uncertainty
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

To change the data uncertainty we increase ``sigma`` and then redefine
the log-Likelihood.

Here we increase the assumed data standard deviation by a factor of of
50! So we are telling the inversion that the data are far less accurate
than they actually are.


.. GENERATED FROM PYTHON SOURCE LINES 897-906

.. code-block:: default


    sigma = 50.0                                     # common noise standard deviation
    Cdinv = np.eye(len(data_y))/(sigma**2)      # inverse data covariance matrix

    def log_likelihood(model):
        y_synthetics = forward(model)
        residual = data_y - y_synthetics
        return -0.5 * residual @ (Cdinv @ residual).T








.. GENERATED FROM PYTHON SOURCE LINES 911-913

Lets return the prior to the original bounds.


.. GENERATED FROM PYTHON SOURCE LINES 913-922

.. code-block:: default


    m_lower_bound = np.ones(4) * (-10.)             # lower bound for uniform prior
    m_upper_bound = np.ones(4) * 10                 # upper bound for uniform prior

    def log_prior(model):    # uniform distribution
        for i in range(len(m_lower_bound)):
            if model[i] < m_lower_bound[i] or model[i] > m_upper_bound[i]: return -np.inf
        return 0.0 # model lies within bounds -> return log(1)








.. GENERATED FROM PYTHON SOURCE LINES 927-955

Your challenge is then to tell CoFI that the Likelihood and prior have
changed and then to rerun the sample, and plot results.

|Upload to Jamboard|

Feel free to start from the code below:

::

   ######## CoFI BaseProblem - update information
   inv_problem.set_log_likelihood(<CHANGE ME>)
   inv_problem.set_log_prior(<CHANGE ME>)

   ######## CoFI Inversion - run it
   inv_5 = Inversion(inv_problem, inv_options_3)
   inv_result_5 = inv_5.run()

   flat_samples = inv_result_5.sampler.get_chain(discard=300, thin=30, flat=True)
   inds = np.random.randint(len(flat_samples), size=100) # get a random selection from posterior ensemble

   print("Resulting samples from changed data uncertainty")
   plot_data()
   plot_models(flat_samples[inds])
   plot_model(x,true_y, "True model", color="darkorange")

.. |Upload to Jamboard| image:: https://img.shields.io/badge/Click%20&%20upload%20your%20results%20to-Jamboard-lightgrey?logo=jamboard&style=for-the-badge&color=fcbf49&labelColor=edede9
   :target: https://jamboard.google.com/d/1ewIkma6uTeNWu7ACEC3vG4J0FNPQZVLdlQLhyeLh-qM/edit?usp=sharing


.. GENERATED FROM PYTHON SOURCE LINES 955-960

.. code-block:: default


    # Copy the template above, Replace <CHANGE ME> with your answer










.. GENERATED FROM PYTHON SOURCE LINES 965-967

The answer is in the next cells if you want to run them.


.. GENERATED FROM PYTHON SOURCE LINES 967-986

.. code-block:: default


    #@title Solution

    ######## CoFI BaseProblem - update information
    inv_problem.set_log_likelihood(log_likelihood)
    inv_problem.set_log_prior(log_prior)

    ######## CoFI Inversion - run it
    inv_5 = Inversion(inv_problem, inv_options_3)
    inv_result_5 = inv_5.run()

    flat_samples = inv_result_5.sampler.get_chain(discard=300, thin=30, flat=True)
    inds = np.random.randint(len(flat_samples), size=100) # get a random selection from posterior ensemble

    print("Resulting samples from changed data uncertainty")
    plot_data()
    plot_models(flat_samples[inds])
    plot_model(x,true_y, "True model", color="darkorange")




.. image-sg:: /tutorials/generated/images/sphx_glr_1_linear_regression_010.png
   :alt: 1 linear regression
   :srcset: /tutorials/generated/images/sphx_glr_1_linear_regression_010.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

      0%|          | 0/10000 [00:00<?, ?it/s]      1%|1         | 123/10000 [00:00<00:08, 1225.74it/s]      2%|2         | 248/10000 [00:00<00:07, 1238.07it/s]      4%|3         | 373/10000 [00:00<00:07, 1240.39it/s]      5%|4         | 498/10000 [00:00<00:07, 1237.06it/s]      6%|6         | 623/10000 [00:00<00:07, 1240.04it/s]      7%|7         | 748/10000 [00:00<00:07, 1242.49it/s]      9%|8         | 873/10000 [00:00<00:07, 1244.43it/s]     10%|9         | 998/10000 [00:00<00:07, 1245.81it/s]     11%|#1        | 1123/10000 [00:00<00:07, 1246.17it/s]     12%|#2        | 1248/10000 [00:01<00:07, 1246.16it/s]     14%|#3        | 1373/10000 [00:01<00:06, 1237.64it/s]     15%|#4        | 1497/10000 [00:01<00:06, 1236.33it/s]     16%|#6        | 1621/10000 [00:01<00:06, 1224.98it/s]     17%|#7        | 1746/10000 [00:01<00:06, 1230.94it/s]     19%|#8        | 1871/10000 [00:01<00:06, 1235.71it/s]     20%|#9        | 1996/10000 [00:01<00:06, 1239.28it/s]     21%|##1       | 2121/10000 [00:01<00:06, 1241.54it/s]     22%|##2       | 2246/10000 [00:01<00:06, 1242.43it/s]     24%|##3       | 2371/10000 [00:01<00:06, 1243.80it/s]     25%|##4       | 2496/10000 [00:02<00:06, 1243.00it/s]     26%|##6       | 2621/10000 [00:02<00:05, 1243.93it/s]     27%|##7       | 2746/10000 [00:02<00:05, 1245.01it/s]     29%|##8       | 2871/10000 [00:02<00:05, 1245.70it/s]     30%|##9       | 2996/10000 [00:02<00:05, 1246.02it/s]     31%|###1      | 3121/10000 [00:02<00:05, 1245.94it/s]     32%|###2      | 3246/10000 [00:02<00:05, 1245.57it/s]     34%|###3      | 3371/10000 [00:02<00:05, 1245.53it/s]     35%|###4      | 3496/10000 [00:02<00:05, 1245.60it/s]     36%|###6      | 3621/10000 [00:02<00:05, 1243.94it/s]     37%|###7      | 3746/10000 [00:03<00:05, 1243.33it/s]     39%|###8      | 3871/10000 [00:03<00:04, 1242.09it/s]     40%|###9      | 3996/10000 [00:03<00:04, 1243.12it/s]     41%|####1     | 4121/10000 [00:03<00:04, 1242.08it/s]     42%|####2     | 4246/10000 [00:03<00:04, 1233.03it/s]     44%|####3     | 4370/10000 [00:03<00:04, 1235.09it/s]     45%|####4     | 4495/10000 [00:03<00:04, 1238.41it/s]     46%|####6     | 4619/10000 [00:03<00:04, 1235.30it/s]     47%|####7     | 4743/10000 [00:03<00:04, 1236.48it/s]     49%|####8     | 4867/10000 [00:03<00:04, 1237.13it/s]     50%|####9     | 4991/10000 [00:04<00:04, 1230.49it/s]     51%|#####1    | 5116/10000 [00:04<00:03, 1235.33it/s]     52%|#####2    | 5241/10000 [00:04<00:03, 1238.59it/s]     54%|#####3    | 5366/10000 [00:04<00:03, 1240.60it/s]     55%|#####4    | 5491/10000 [00:04<00:03, 1242.51it/s]     56%|#####6    | 5616/10000 [00:04<00:03, 1242.83it/s]     57%|#####7    | 5741/10000 [00:04<00:03, 1244.11it/s]     59%|#####8    | 5866/10000 [00:04<00:03, 1244.51it/s]     60%|#####9    | 5991/10000 [00:04<00:03, 1242.56it/s]     61%|######1   | 6116/10000 [00:04<00:03, 1243.11it/s]     62%|######2   | 6241/10000 [00:05<00:03, 1244.28it/s]     64%|######3   | 6366/10000 [00:05<00:02, 1243.18it/s]     65%|######4   | 6491/10000 [00:05<00:02, 1243.80it/s]     66%|######6   | 6616/10000 [00:05<00:02, 1244.44it/s]     67%|######7   | 6741/10000 [00:05<00:02, 1245.05it/s]     69%|######8   | 6866/10000 [00:05<00:02, 1243.62it/s]     70%|######9   | 6991/10000 [00:05<00:02, 1244.60it/s]     71%|#######1  | 7116/10000 [00:05<00:02, 1245.47it/s]     72%|#######2  | 7241/10000 [00:05<00:02, 1243.77it/s]     74%|#######3  | 7366/10000 [00:05<00:02, 1243.54it/s]     75%|#######4  | 7491/10000 [00:06<00:02, 1243.03it/s]     76%|#######6  | 7616/10000 [00:06<00:01, 1243.37it/s]     77%|#######7  | 7741/10000 [00:06<00:01, 1243.07it/s]     79%|#######8  | 7866/10000 [00:06<00:01, 1243.63it/s]     80%|#######9  | 7991/10000 [00:06<00:01, 1243.51it/s]     81%|########1 | 8116/10000 [00:06<00:01, 1229.10it/s]     82%|########2 | 8241/10000 [00:06<00:01, 1232.81it/s]     84%|########3 | 8366/10000 [00:06<00:01, 1236.67it/s]     85%|########4 | 8491/10000 [00:06<00:01, 1238.65it/s]     86%|########6 | 8616/10000 [00:06<00:01, 1240.69it/s]     87%|########7 | 8741/10000 [00:07<00:01, 1242.89it/s]     89%|########8 | 8866/10000 [00:07<00:00, 1243.34it/s]     90%|########9 | 8991/10000 [00:07<00:00, 1239.73it/s]     91%|#########1| 9115/10000 [00:07<00:00, 1239.13it/s]     92%|#########2| 9240/10000 [00:07<00:00, 1240.30it/s]     94%|#########3| 9365/10000 [00:07<00:00, 1238.17it/s]     95%|#########4| 9490/10000 [00:07<00:00, 1240.35it/s]     96%|#########6| 9615/10000 [00:07<00:00, 1240.40it/s]     97%|#########7| 9740/10000 [00:07<00:00, 1240.34it/s]     99%|#########8| 9865/10000 [00:07<00:00, 1240.27it/s]    100%|#########9| 9990/10000 [00:08<00:00, 1239.52it/s]    100%|##########| 10000/10000 [00:08<00:00, 1240.80it/s]
    Resulting samples from changed data uncertainty




.. GENERATED FROM PYTHON SOURCE LINES 991-1039

Challenge: Change the number of walkers / steps in the McMC algorithm (optional)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Now lets decrease the number of steps performed by the McMC algorithm.
It will be faster but perform less exploration of the model parameters.

We suggest you reduce the number of steps taken by all 32 random walkers
and see how it affects the posterior ensemble.

|Upload to Jamboard|

You can start from code template below:

::

   # change number of steps
   nsteps = <CHANGE ME>              # instead of 10000

   # change number of walkers
   nwalkers = <CHANGE ME>            # instead of 32
   walkers_start = np.zeros(nparams) + 1e-4 * np.random.randn(nwalkers, ndim)

   # let's return to the old uncertainty settings
   sigma = 1.0                                     # common noise standard deviation
   Cdinv = np.eye(len(data_y))/(sigma**2)      # inverse data covariance matrix

   ######## CoFI BaseProblem - update information
   inv_problem.set_walkers_starting_pos(walkers_start)

   ######## CoFI InversionOptions - get a different tool
   inv_options_3.set_params(nsteps=nsteps)

   ######## CoFI Inversion - run it
   inv_6 = Inversion(inv_problem, inv_options_3)
   inv_result_6 = inv_6.run()

   ######## CoFI InversionResult - plot result
   flat_samples = inv_result_6.sampler.get_chain(discard=300, thin=30, flat=True)
   inds = np.random.randint(len(flat_samples), size=10) # get a random selection from posterior ensemble

   print(f"Inference results from {nsteps} steps and {nwalkers} walkers")
   plot_data()
   plot_models(flat_samples[inds])
   plot_model(x,true_y, "True model", color="darkorange")

.. |Upload to Jamboard| image:: https://img.shields.io/badge/Click%20&%20upload%20your%20results%20to-Jamboard-lightgrey?logo=jamboard&style=for-the-badge&color=fcbf49&labelColor=edede9
   :target: https://jamboard.google.com/d/1vAm3dpaI4UTZiFXzb6vEku8AlVWUw7PRxz8KJk-dVf8/edit?usp=sharing


.. GENERATED FROM PYTHON SOURCE LINES 1039-1044

.. code-block:: default


    # Copy the template above, Replace <CHANGE ME> with your answer










.. GENERATED FROM PYTHON SOURCE LINES 1046-1079

.. code-block:: default


    #@title Solution

    # change number of steps
    nsteps = 400              # instead of 10000

    # change number of walkers
    nwalkers = 30             # instead of 32
    walkers_start = np.zeros(nparams) + 1e-4 * np.random.randn(nwalkers, ndim)

    # let's return to the old uncertainty settings
    sigma = 1.0                                     # common noise standard deviation
    Cdinv = np.eye(len(data_y))/(sigma**2)      # inverse data covariance matrix

    ######## CoFI BaseProblem - update information
    inv_problem.set_walkers_starting_pos(walkers_start)

    ######## CoFI InversionOptions - get a different tool
    inv_options_3.set_params(nsteps=nsteps, nwalkers=nwalkers)

    ######## CoFI Inversion - run it
    inv_6 = Inversion(inv_problem, inv_options_3)
    inv_result_6 = inv_6.run()

    ######## CoFI InversionResult - plot result
    flat_samples = inv_result_6.sampler.get_chain(discard=300, thin=30, flat=True)
    inds = np.random.randint(len(flat_samples), size=10) # get a random selection from posterior ensemble

    print(f"Inference results from {nsteps} steps and {nwalkers} walkers")
    plot_data()
    plot_models(flat_samples[inds])
    plot_model(x,true_y, "True model", color="darkorange")




.. image-sg:: /tutorials/generated/images/sphx_glr_1_linear_regression_011.png
   :alt: 1 linear regression
   :srcset: /tutorials/generated/images/sphx_glr_1_linear_regression_011.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

      0%|          | 0/400 [00:00<?, ?it/s]     32%|###1      | 127/400 [00:00<00:00, 1262.64it/s]     64%|######4   | 256/400 [00:00<00:00, 1274.45it/s]     96%|#########6| 385/400 [00:00<00:00, 1279.17it/s]    100%|##########| 400/400 [00:00<00:00, 1274.54it/s]
    Inference results from 400 steps and 30 walkers




.. GENERATED FROM PYTHON SOURCE LINES 1084-1089

--------------

Watermark
---------


.. GENERATED FROM PYTHON SOURCE LINES 1089-1095

.. code-block:: default


    watermark_list = ["cofi", "numpy", "scipy", "matplotlib", "emcee", "arviz"]
    for pkg in watermark_list:
        pkg_var = __import__(pkg)
        print(pkg, getattr(pkg_var, "__version__"))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    cofi 0.1.2.dev22
    numpy 1.21.6
    scipy 1.9.1
    matplotlib 3.5.3
    emcee 3.1.2
    arviz 0.12.1





.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  29.359 seconds)


.. _sphx_glr_download_tutorials_generated_1_linear_regression.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example


    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: 1_linear_regression.py <1_linear_regression.py>`

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: 1_linear_regression.ipynb <1_linear_regression.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
