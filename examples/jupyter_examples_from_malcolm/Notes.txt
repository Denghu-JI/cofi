
Malcolm's notes on the inversion tutorial python notebooks.

We have produced a dozen or so inversion tutorials notebooks that depend on various pieces of other software, e.g. pure python, C++ and ancient Fortran.  

Below is my interpretation of several of these in terms of proposed generic elements of an inverse problem. It is hoped that collectively they provide a test of whether characterising inverse problems in this way makes sense.

The action item is that Peter will take these and evaluate whether they can collectively be put in a common framework exploiting the common elements, perhaps by building a prototype CoFI interface which captures these examples.

------------------------------------------------------------------------------------

Elements of the common framework for inference:

Model - a class 
	properties - contains all information necessary to define a model. It includes a set of values defining a particular discrete model 
	in terms of vectors, integers, categories, model vector m. This would be the model vector within an inversion. It may have functions which 
	convert the model parameter values used in inversion to some other format more suited to interpretation, for example 
	multiplying model coefficients by basis functions to produce a 1D/2D/3D field of earth model parameters. Multiple/many of these might be 	generated during an inversion application.
	While we anticipate standard properties, in principle a user has the option to add functionality to this class, e.g. by tagging auxiliary 	problem dependent information to each model. 


Data - a class
	properties - contains all information necessary to define observed data and its noise characteristics. It includes a set of values 
	defining a particular set of discrete data used in an inversion, model vector d. It may include auxiliary data, e.g. seismic receiver details 	needed to make use of the data, e.g. by a forward solver. This class is to be used to represent both the observations and the model 		predictions generated by a forward solver with are compared to observations. Functions will exist to recover information and manipulate 
        data sets consistent with the application. Multiple/many of these might be generated during an inversion application. 
	While we anticipate standard properties, in principle a user has the option to add functionality to this class, e.g. by tagging auxiliary 	problem dependent information to each model. 

Objective - a class
	Inputs - a realisation of a model and a data class.
	Outputs - a scalar or several scalars to be used by the inverse method to define the merit or otherwise of a given model, 
                  optionally first and second derivatives of the objective with respect to the model parameters within the model class, 
        This has knowledge of and calls the forward solver to make data predictions from the input model.
	Typically passed to the inverse solver and used by it.
	Examples include - A least squares misfit between inputs; a negative log Likelihood; the log of the posterior probability distribution; some 	other measure of distance between observations and model predictions.
	Notes: All interface of the inverse solver to the forward solver is indirectly via this class. In a Bayesian sampling problem 
               it would typically return the log of the PPD. 
	While we anticipate standard templates, in principle a user has the option to supply this class to represent whatever prior/likelihood 
        or misfit function desired. 

Forward solver - something that takes a data class and a model class and produces data predictions in the form of a data class realisation.
                 embeds the forward physics. 

Inverse solver - something that operates on the data class and optionally takes in a model class and a set of inversion control choices, e.g. 			 methodology and related inputs. 
		 Produces an output class whose nature depends on the input controls. Could be an optimal model plus uncertainty; an ensemble 
		 of model class outputs. 
		 Has knowledge of the model class structure, e.g. number and nature of unknowns.
		 Makes use of the objective class. Might not be directly aware of the data class, but only interact with it though the objective 		 function class.
       		 It is anticipated that and diagnostic information on performance would also be returned.

Since an inversion may involve many models (model class) and model predictions (data class) space overheads can be minimized by using pointers to static information within each object, eg. seismic receiver positions, moment tensors for earthquakes etc.  

Comments
In writing this it seems clear that the nature of operations on the model and data classes as well as auxiliary data will mostly be application dependent, although there may also be some generic ones that we should assume exist, i.e. .get_values() etc. 

An anticipated inversion might proceed as follows:
	1. Set up a model class
	2. Set up data class 
	3. Input observations into the data class to create a special 'observed' realisation of the data class.
	4. Create a Forward solver
        5. Create an objective class.
	6. Run the inverse solver.
        7. User takes the inverse solver's output and analyses it remotely or within a programatic interface. 

Typically we might do 1,2,4,5 only once. A user might repeat 3,6,7 many times.

------------------------------------------------------------------------------------

How do these classes relate to the python inversion tutorial notebooks?

------------------------------------------------------------------------------------
File: 'S4.3 - Simple Probabilistic sampling.ipynb'
------------------------------------------------------------------------------------

This is an example of a simple 2D McMC probabilistic sampler of a mixture of three Gaussians.

Model: 2 scalar variables (x,y) together with bounds on each parameter (equivalent to a uniform prior PDF between bounds).

Data: 6 scalar values representing the means of the three Gaussian PDFs (0.6,0.6),(0.8,0.2),(0.2,0.3). 
      Auxiliary data are the three scalar weights and the three 2x2 covariance matrices.
      Note here the model and the data values are in the same 2D space.

Objective: the PDF of a sum of Gaussians evaluated at any input model (x,y point). In general this should be recast as the log of the target as this is safer in most applications. here we keep it simple. The `target' routine in the notebook returns the PDF rather than the log and the solver operates on this.

Forward solver: Can be considered to be the identity, since the input model values are directly compared to the three data pairs in the mixture model.  		i.e. the model predictions are equal to the model. (This situation may also arise in some examples of regression.)

Inverse solver: a simple Metropolis McMC sampler which generates an ensemble of (x,y) points whose density approximately follow the multimodal target PDF.

Comments: This is pure python and requires only standard libraries.
	  The lack of a forward problem is immaterial here as there is no reference to one in the objective function. 
	  In larger applications we might not always return the entire ensemble, but have the option to specify properties of an ensemble, 
	  which are calculated on the fly and returned without the ensemble which is discarded. This is typically done when models have high 
	  dimension and Markov chains are long.

------------------------------------------------------------------------------------
File: `S5.1 - Fully nonlinear search Rosenbrock.ipynb'
------------------------------------------------------------------------------------

This is an example of a 2D deterministic minimisation of the Rosenbrock function using randomised sampling in a contracting box .

Model: 2 scalar variables (x,y) together with bounds on each parameter.

Data: No actual data values here as the Rosenbrock is a direct function of the model parameters. It should be Null?

Objective: Takes any pair of model parameters and evaluates the analytical Rosenbrock function. 
	   No need to refer to a forward solver. No need to use data.

Forward solver: Can be considered to be the identity, since the input model values are directly used by the objective function, i.e. the model 			predictions are equal to the model.

Inverse solver: At each iteration uniform random samples of (x,y) pairs are drawn in a box. The lowest value of the Rosenbrock function 
		is recorded. At the next iteration the box is shrunk and centred on the lowest known value of the objective, and the process 	
		restarted. A fixed total number of iterations are used and results returned.

Comments: This is pure python and requires only standard libraries.

------------------------------------------------------------------------------------
File: `S5.2 - Fully nonlinear search Waveforms.ipynb'
------------------------------------------------------------------------------------

This is an example of finding a 1D seismic shear wave model that best predicts an observed (noisy) seismic receiver function. 

Two methods are used: 
The first is deterministic grid search over a subset of model parameters (to produce contour maps of misfit) and profiles of misfit along chosen axes. 
The second is optimisation via generation of randomised Gaussian samples, where the centre of the cloud iteratively updates. 

This is a highly nonlinear minimisation problem whet the objective function is multi-modal as a function of the interface depth/later thickness parameters. It is usually near quadratic as a function of the Shear wave velocity parameters (see profile figure).

Model: 	There is a five layer seismic model with up to 3 associated model parameters per layer, (interface depths, Vs, Vp/Vs ratios one per later) 
	making up to a 15 dimensional model space. Only a subset are ever varied in the examples, i.e. 1 parameter for the misfit profile examples, 
	2 parameters for the grid search and Gaussian random sampling. Simple bounds are placed on each model parameter for grid search.

Data: 	Amplitudes of a single observed receiver function with added noise as a function of time. Here it is synthetic and generated 
	internally for these test examples. Auxiliary data are the time axis values associated with the amplitudes, making up the RF.

Objective: Takes in a full set of 15 model parameters and evaluates the predicted receiver function using the forward solver (rf.calc) and 
	   returns the squared waveform difference. No derivatives are returned.

Forward solver: Routine rf.calc() takes in the 1D velocity model and produces a predicted receiver function. Other dependent parameters are 
		exposed through the routine rf.calc but kept fixed in the examples.

Inverse solver: Grid search: Evaluate the objective function for a predetermined set of models, e.g. with one or two parameters uniformly varying 		between bounds. Returns the ensemble for plotting. Keeps a record of lowest objective function. 
 		Gaussian sampling: Takes a supplied initial starting (1D Earth) model and generates Gaussian random deviates about this model 
		over two chosen parameters (an interface depth and a velocity in the example). Records the best best fit model and iteratively 
		recenters this cloud and repeats till convergence (which is a fixed number of iterations). Examples of this process converging 
                to correct solution and incorrect solution are shown (depends on starting model).


Comments: The forward model here is in Fortran (77!) which is converted to a python module `rfc.cpython-37m-darwin.so' using f2py. 
          The Fortran source is in `inversionCourse/rfc'. Instructions are followed in `inversionCourse/rfc/READMME' to 
	  produce `rfc.cpython-37m-darwin.so'. Python interface routine `inversionCourse/rf.py' is loaded in the notebook and provides a convenient 
	  interface. `inversionCourse/rf.py' loads `rfc.cpython-37m-darwin.so' via `import rfc'.


------------------------------------------------------------------------------------
File: `S6.1 - TransD sampling polynomial regression.ipynb'
------------------------------------------------------------------------------------

This is a trans-D regression example of finding a curve that best predicts an observed (noisy) set of (x,y) points. 

It uses trans-D Bayesian sampling as described in various papers of ours, and Hawkins thesis.


Model: 	A transD model with model parameters are polynomial coefficients within a variable numbers of partitions across the x axis. The locations of 	the partition boundaries are/can be also unknowns. The number of partitions and polynomial order may be variable from 1 partition to 
	a chosen upper bound and from 1 coefficient (i.e. constant) to up to fifth order polynomial per partition. 

Data: 	Observed pairs of (x,y) data to fit.

Objective: Internally set to negative log Gaussian likelihood  of residual plus log of prior of model parameters. 
	   Residual = y observed - y predicted by polynomial in partition corresponding to observed x. There is a log of prior corresponding to both 	   polynomial coefficients and interface positions. Typically these are both uniform within bounds. 

Forward solver: Takes in cell interface positions and polynomial coefficients within each partition and produces a piecewise continuous function y(x) 		which is compared to the observations by the Objective function.

Inverse solver: Trans-D Bayesian sampling of the multi-dimensional posterior PDF, as described in papers below. The output is a posterior ensemble of 
		coefficients and interface locations/Voronoi cell nuclei defining piecewise polynomial coefficients. The average of these is typically 
		a smooth function, although the same approach can be used for change point modelling (detecting discontinuities in data sets) if the 		polynomial is restricted to zeroth order (i.e. a constant = one unknown).

Details of this approach can be found in. 
Trans-dimensional surface reconstruction with alternate parameterizations,
Hawkins, R., Bodin, T., Sambridge, M., Choblet, G., Hussin, L., 2019. Geochem. Geophys. Geosys., 20, 505-529, doi:10.1029/2018GC008022.

Transdimensional inference in the geosciences
Sambridge, M., Bodin, T., Gallagher, K., Tkalčić, H., 2013. Phil. Trans. R. Soc. A, 371, 20110547, doi:10.1098/rsta.2011.0547.


Comments: This uses an external code to do all of the work here. Its written in C++ (by Rgys Hawkins) and has an optional python interface, which we make use of, to supply data, make choices and runs cases.  The underlying C++ code has everything in it. The rjmcmc is design as a simple tool for the user. The package is a tar ball with instructions for complication and building of python module. This is in `Tars/RJMCMC-1.1.0.tar'. In the notebook it is assumed that package rjmcmc exists. the package contains stand alone python routines to perform these are more complex trans-D regression task. Its a more challenging example because it uses an external C++ code. 

I would very much like to get this into CoFI because it is a sophisticated regression solver and also a change point solver. It is a bona fide example of the type of thing that it would be good to have within CoFI, but it may be a considerable challenge because the objective and inverse solver are not directly exposed to the user in these examples. The python interface tool being used here was designed to be as a simple as possible. However underneath it is a C++ library intended to be put in ones own code, so in principle it will all be there and translatable into a CoFI framework. See directory `RJMCMC-1.1.0/python/tutorial' for more sophisticated examples. There is also a manual in directory `doc'. Not clear to me whether this should be in the too hard basket for now? 

Note: There are no examples of gradient based optimisation here, although I have some.
